{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Natural Language Processing: Topic Modeling with NMF\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "Data Science Cohort Live NYC 2023\n",
    "<p>Phase 4</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# standard packages for data analysis and NLP\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "#visualization packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLP modules we will use for text normalization\n",
    "import re #regex \n",
    "import nltk # the natural language toolkit\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "# feature construction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #use this to create BoW matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Some special packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#pip install pyLDAvis==3.3.1\n",
    "\n",
    "#conda install -c conda-forge pyldavis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.sklearn # a specialized package for topic model visualization\n",
    "\n",
    "#modeling and dimensionality reduction for visuaization\n",
    "from sklearn.decomposition import NMF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Taking a look at our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Load in our Covid-19 Tweet dataset. \n",
    "- In general: twitter REST API or tweepy to download tweets.\n",
    "- We will load from csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvid_dataset_orig = pd.read_csv(\"Data/Corona_NLP_train.csv\", encoding='latin-1')\n",
    "cvid_dataset_orig = cvid_dataset_orig.rename(columns = {'UserName': 'user_name', 'ScreenName': 'screen_name', 'Sentiment': 'sentiment', 'OriginalTweet': 'text', 'TweetAt': 'date', 'Location': 'location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cvid_dataset = deepcopy(cvid_dataset_orig).drop(columns = ['sentiment'])\n",
    "cvid_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are no nulls in the  actual text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvid_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take a closer look at some of our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvid_dataset['text'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvid_dataset['text'].loc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvid_dataset['text'].loc[239]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cvid_dataset['text'].loc[2300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What are some potential cleaning tasks that you can identify?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Preprocess tweet text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Will be our workhorse function for text cleaning and preprocesses a single tweet. \n",
    "\n",
    "- Regex: removes hashtags, mentions, urls, line break special characters, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# additional argument sets cut off minimum length for tokenized text at which function converts to null string.\n",
    "def process_tweet(tweet_text, min_length):\n",
    "    \n",
    "    # get common stop words that we'll remove during tokenization/text normalization\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    #initialize lemmatizer\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # helper function to change nltk's part of speech tagging to a wordnet format.\n",
    "    def pos_tagger(nltk_tag):\n",
    "        if nltk_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif nltk_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif nltk_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif nltk_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:         \n",
    "            return None\n",
    "   \n",
    "\n",
    "    # lower case everything\n",
    "    tweet_lower = tweet_text.lower()\n",
    "\n",
    "    #remove mentions, hashtags, and urls, strip whitspace and breaks\n",
    "    tweet_lower = re.sub(r\"@[a-z0-9_]+|#[a-z0-9_]+|http\\S+\", \"\", tweet_lower).strip().replace(\"\\r\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "    \n",
    "    \n",
    "    # remove stop words and punctuations \n",
    "    tweet_norm = [x for x in word_tokenize(tweet_lower) if ((x.isalpha()) & (x not in stop_words)) ]\n",
    "\n",
    "    #  POS detection on the result will be important in telling Wordnet's lemmatizer how to lemmatize\n",
    "    \n",
    "    # creates list of tuples with tokens and POS tags in wordnet format\n",
    "    wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tag(tweet_norm))) \n",
    "\n",
    "    # now we are going to have a cutoff here. any tokenized cocument with length < min length will be removed from corpus\n",
    "    if len(wordnet_tagged) <= min_length:\n",
    "        return ''\n",
    "    else:\n",
    "         # rejoins lemmatized sentence \n",
    "         tweet_norm = \" \".join([wnl.lemmatize(x[0], x[1]) for x in wordnet_tagged if x[1] is not None])\n",
    "         return tweet_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Apply our text normalization and delete empty tweets. Might take a minute or two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# anything with no of tokens <= 10 is likely junk. apply has additional args parameter to pass in function arguments.\n",
    "cvid_dataset['text'] = cvid_dataset['text'].apply(process_tweet, args = [10])\n",
    "\n",
    "#our processing created some empty documents, so we should drop these.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# some documents are short enough cleaning may have wiped it out.\n",
    "cvid_dataset_new = cvid_dataset[cvid_dataset['text'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Creating our Bag of Words Term-Document Matrix\n",
    "\n",
    "Apply tf-idf vectorizer to transform the preprocessed text into a term-document matrix. \n",
    "$$ w_{ij} = tf_{ij}\\log(\\frac{N}{df_i}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "corpus = cvid_dataset_new['text']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(corpus)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "X_train is in sparse matrix format (saves space/time), can't view directly. \n",
    "\n",
    "- Number of unique tokens in our term frequency matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once the data is in the right form, scikit learn makes NMF topic modeling is as easy as this:\n",
    "- fit with 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "topic_model = NMF(n_components = 5)\n",
    "topic_model.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Remember that: $$ X = WH $$\n",
    "\n",
    "Our model has fitted W and H, so we can get these components independently.\n",
    "- $ W $ encodes the importance of each token in the fitted topics. \n",
    "- $ H $ encodes the weight of the fitted topics for each document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# to get H\n",
    "H = topic_model.transform(X_train) # transform document into topic vector representation\n",
    "\n",
    "# to get W \n",
    "W = topic_model.components_ # word component weights for each topic\n",
    "\n",
    "print(\"Shape of H is \" + str(H.shape))\n",
    "print(\"Shape of W is \" + str(W.shape))\n",
    "print(\"Shape of X_train is \" + str(X_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Remember that there are 29,178 tweets in our dataset. \n",
    "- Vectorizer created 25,788 features with varying importance. \n",
    "\n",
    "\n",
    "**Dimensions and our interpretations of W and H make sense.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The W matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's take a look at the tokens (columns of W) with highest weight for each topic (columns) in W:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# weight for given token\n",
    "print(W[0])\n",
    "print(len(W[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for index,topic in enumerate(W):\n",
    "    print(f'THE TOP 25 WORDS FOR TOPIC #{index}')\n",
    "    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-25:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It's often helpful to make a bar visualization of the most relevant token weights for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture topic_word_plot\n",
    "def plot_top_words(W, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 8), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(W):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=15)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=25)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()\n",
    "\n",
    "n_top_words = 20\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "plot_top_words(W, tfidf_feature_names, n_top_words, \"Topics in NMF model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "topic_word_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PyLDAvis: an excellent tool for visualizing topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "vis = pyLDAvis.sklearn.prepare(topic_model, X_train,vectorizer)\n",
    "pyLDAvis.display(vis)\n",
    "pyLDAvis.save_html(vis, 'nmf_topics.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Based off of this, let's label these topics with names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "topic_name_dict = {0: 'essential_worker', 1: 'oil_market_crisis', 2: 'supply_shortage', 3: 'sanitizing_products', 4: 'shopping'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The H matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So far: word distributions in each topic and evaluated topic similarity/difference. \n",
    "- This was all in the matrix $W$. \n",
    "\n",
    "But what about $H$? \n",
    "- $H$ contains information about breakdowns of topics in each document. Let's see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# takes in list of documents and plots topic weight vectors for each document\n",
    "def tweet_topbreakdown(locator):\n",
    "\n",
    "    print(cvid_dataset_orig.loc[locator].text)\n",
    "    int_index = cvid_dataset_new.index.get_loc(locator)\n",
    "\n",
    "    topic_keys = topic_name_dict.values()\n",
    "    zipped_tuple = list(zip(topic_keys, list(H[int_index,:])))\n",
    "\n",
    "    topic_breakdown = pd.DataFrame(zipped_tuple, columns = ['Topic', 'Weight']).set_index(['Topic'])\n",
    "    topic_breakdown['Normalized weight'] = topic_breakdown['Weight']/topic_breakdown['Weight'].sum()\n",
    "\n",
    "    sns.barplot(y = topic_breakdown.index, x = 'Normalized weight', data = topic_breakdown)\n",
    "    plt.title(\"Distribution of topics for tweet no. \" + str(locator))\n",
    "    plt.show()\n",
    "\n",
    "    return display(topic_breakdown)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tweet_loc_list = [5,10,115, 320]\n",
    "g = list(map(tweet_topbreakdown, tweet_loc_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TSNE: A way to visualize our documents by topic in 2D\n",
    "\n",
    "TSNE (or t-distributed stochastic neighbor embedding) is a way to take high dimensional data and embed it into 2D for visualization. The technique is good at helping to identify clusters or neighborhoods in text data. \n",
    "\n",
    "Scale/distances dont mean too much, but clustering and closeness does.\n",
    "\n",
    "Stochastic because each time you run you get different 2D embeddings.\n",
    "\n",
    "Scikit-learn makes it easy, only has a fit_transform method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE # T-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state=42, learning_rate=100)\n",
    "tsne_trans = tsne.fit_transform(H)\n",
    "tsne_trans = pd.DataFrame(tsne_trans, columns = ['TSNE1', 'TSNE2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# for each document takes the topic with highest weight and assigns document to this class -- hard clustering.\n",
    "tsne_trans['class'] = np.argmax(H, axis = 1)\n",
    "tsne_trans['class'] = tsne_trans['class'].replace(topic_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "sns.scatterplot(x = 'TSNE1', y = 'TSNE2', hue = 'class', data = tsne_trans, palette = 'tab10')\n",
    "plt.title('Visualization of COVID-19 tweet topic segmentation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Finishing up and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have found a higher level representation of the data, encoded in $H$:\n",
    "\n",
    "- The algorithm has learned concepts and now we want to do analytics on these concepts.\n",
    "\n",
    "First, let's reform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "H_repres_norm = pd.DataFrame(H, columns = topic_name_dict.values(), index = cvid_dataset_new.index)\n",
    "H_repres_norm = H_repres_norm.divide(H_repres_norm.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "H_repres_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's join this to the rest of the dataset and clean up a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "embedded_tweets_df = cvid_dataset.join(H_repres_norm).dropna()\n",
    "embedded_tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "embedded_tweets_df.to_csv('cvid_embedded_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What kind of tasks could we do or things could we learn from the data in this representation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "95508d1f9e276930ee7bf977622471e4c9572bc985693ed5ef89eacdfe75f4af"
  },
  "kernelspec": {
   "display_name": "dplearn",
   "language": "python",
   "name": "dplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
